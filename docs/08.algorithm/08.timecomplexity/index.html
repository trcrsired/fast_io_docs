<!DOCTYPE html>
<html lang="en">
<head>
  <meta charset="UTF-8">
  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <title>Ch8.8: Time &amp; Space Complexity - C++ Tutorial</title>
  <link rel="stylesheet" href="/style.css">
  <link rel="manifest" href="/manifest.json">
  <link rel="icon" type="image/webp" href="/icons/logo.webp">
</head>
<body>
  <main>
    <h1>Ch8.8: Time &amp; Space Complexity</h1>

    <section>
      <h2>Overview</h2>

      <p>
        Time and space complexity describe how the running time and memory usage
        of an algorithm grow as the input size grows. In this chapter we focus
        on <strong>time complexity</strong>, but we also explain how space
        complexity usually gives a <strong>lower bound</strong> on time.
      </p>

      <p>You will learn:</p>

      <ul>
        <li>a high-level view of the RAM model of computation</li>
        <li>why space complexity is usually a lower bound on time complexity</li>
        <li>the meaning of <code>O</code>, <code>&Omega;</code>, and <code>&Theta;</code></li>
        <li>basic &Theta; computations</li>
        <li>how to analyze simple loops</li>
        <li>how to analyze ranges algorithms</li>
        <li>why projection cost matters</li>
        <li>why recursive Fibonacci has exponential time complexity</li>
        <li>why games like Go are very hard for computers</li>
        <li>how to compute recursive time complexity using the Master Theorem (appendix)</li>
      </ul>
    </section>

    <section>
      <h2>0. The model of computation</h2>

      <p>
        When we talk about time complexity, we are always talking about the
        running time of an algorithm on a particular abstract machine model.
        The most common model is the <strong>RAM model</strong> (Random Access Machine).
      </p>

      <p>The RAM model assumes:</p>

      <ul>
        <li>each basic operation (addition, comparison, assignment) takes constant time</li>
        <li>memory access to any element takes constant time</li>
        <li>the machine has unlimited memory</li>
      </ul>

      <p>
        These assumptions make it possible to describe algorithms using simple
        notations like <code>&Theta;(n)</code> and <code>O(n&nbsp;ln&nbsp;n)</code>.
      </p>
    </section>

    <section>
      <h2>1. Space complexity as a lower bound</h2>

      <p>
        Space complexity measures how much memory an algorithm uses as a function
        of the input size <code>n</code>. Time complexity measures how long it runs.
      </p>

      <p>
        Space complexity is usually a <strong>lower bound</strong> on time complexity.
        If an algorithm uses <code>S(n)</code> space, it must spend at least
        <code>&Omega;(S(n))</code> time to allocate, initialize, read, or manipulate
        that space.
      </p>

      <p>Examples:</p>

      <ul>
        <li>If an algorithm stores <code>n</code> elements → at least <code>&Omega;(n)</code> time.</li>
        <li>If an algorithm uses <code>&Theta;(1)</code> space → time may be larger, but never smaller than <code>&Omega;(1)</code>.</li>
      </ul>
    </section>

    <section>
      <h2>2. The three basic notations</h2>

      <p>
        Time complexity uses three related notations:
      </p>

      <ul>
        <li><strong>O(f(n))</strong>: an upper bound</li>
        <li><strong>&Omega;(f(n))</strong>: a lower bound</li>
        <li><strong>&Theta;(f(n))</strong>: a tight bound</li>
      </ul>

      <p>
        We use them precisely:
      </p>

      <ul>
        <li><strong>&Theta;</strong> only when the bound is tight</li>
        <li><strong>O</strong> when only an upper bound is known (e.g., <code>std::ranges::sort</code>)</li>
        <li><strong>&Omega;</strong> when only a lower bound is known</li>
      </ul>
    </section>

    <section>
      <h2>3. Basic &Theta; computations</h2>

      <pre><code class="language-cpp">
Θ(3n^3) = Θ(n^3)
      </code></pre>

      <pre><code class="language-cpp">
Θ(n^3 + 10000000000n^2) = Θ(n^3)
      </code></pre>

      <p>
        Only the highest-order term matters.
      </p>
    </section>

    <section>
      <h2>4. Complexity of simple loops</h2>

      <pre><code class="language-cpp">
for (::std::size_t i{}; i != n; ++i) {
    // constant-time work
}
      </code></pre>

      <p><strong>&Theta;(n)</strong></p>

      <pre><code class="language-cpp">
for (::std::size_t i{}; i != n; ++i)
    for (::std::size_t j{}; j != n; ++j)
        ;
      </code></pre>

      <p><strong>&Theta;(n^2)</strong></p>
    </section>

    <section>
      <h2>5. Linear-time algorithms</h2>

      <p>Examples:</p>

      <ul>
        <li><code>find</code>, <code>find_if</code></li>
        <li><code>count</code>, <code>count_if</code></li>
        <li><code>transform</code></li>
        <li><code>remove</code>, <code>remove_if</code></li>
        <li><code>accumulate</code>, <code>reduce</code></li>
      </ul>

      <pre><code class="language-cpp">
::std::size_t ones = ::std::ranges::count(v, 1);
      </code></pre>

      <p><strong>&Theta;(n)</strong></p>
    </section>

    <section>
      <h2>6. Binary search: O(ln&nbsp;n)</h2>

      <pre><code class="language-cpp">
auto it = ::std::ranges::lower_bound(v, value);
      </code></pre>

      <p>
        Each iteration halves the search space → <strong>O(ln&nbsp;n)</strong>.
      </p>
    </section>

    <section>
      <h2>7. Sorting: O(n&nbsp;ln&nbsp;n)</h2>

      <pre><code class="language-cpp">
::std::ranges::sort(v, {}, &amp;Item::key);
      </code></pre>

      <p>
        The C++ standard guarantees only an upper bound → <strong>O(n&nbsp;ln&nbsp;n)</strong>.
      </p>
    </section>

    <section>
      <h2>8. Why projection cost matters</h2>

      <pre><code class="language-cpp">
::std::ranges::sort(v, {}, &amp;Item::key);
      </code></pre>

      <pre><code class="language-cpp">
::std::ranges::sort(v, {}, [](Item const&amp; x) {
    return expensive_computation(x);
});
      </code></pre>

      <p>
        Expensive projections dominate runtime.
      </p>
    </section>

    <section>
      <h2>9. Numeric algorithms</h2>

      <pre><code class="language-cpp">
::std::size_t total = ::std::transform_reduce(
    v.begin(), v.end(),
    ::std::size_t{},
    ::std::plus&lt;&gt;(),
    &amp;Item::value
);
      </code></pre>

      <p><strong>&Theta;(n)</strong></p>
    </section>

    <section>
      <h2>10. Recursive Fibonacci: exponential time</h2>

      <pre><code class="language-cpp">
::std::size_t fib(::std::size_t n) {
    if (n &lt; 2) return n;
    return fib(n - 1) + fib(n - 2);
}
      </code></pre>

      <p><strong>&Theta;(2^n)</strong></p>
    </section>

    <section>
      <h2>11. NP problems and hard search spaces</h2>

      <p>
        Many real-world problems have exponential search spaces and are believed
        to require exponential time to solve exactly.
      </p>
    </section>

    <section>
      <h2>12. Why Go is extremely hard for computers</h2>

      <pre><code class="language-cpp">
200^200
      </code></pre>

      <p>
        Go has an exponential game tree → brute force is impossible.
      </p>
    </section>

    <section>
      <h2>13. Why AlphaGo succeeded</h2>

      <p>
        AlphaGo did not solve Go exactly. It used:
      </p>

      <ul>
        <li>deep neural networks</li>
        <li>policy networks</li>
        <li>Monte Carlo Tree Search</li>
      </ul>

      <p>
        It approximated good moves instead of exploring the full exponential tree.
      </p>

      <div class="video-container">
        <iframe
          width="560"
          height="315"
          src="https://www.youtube.com/embed/WXuK6gekU1Y"
          title="AlphaGo - The Movie | Full award-winning documentary"
          frameborder="0"
          allow="accelerometer; autoplay; clipboard-write; encrypted-media; gyroscope; picture-in-picture"
          allowfullscreen>
        </iframe>
      </div>
    </section>

    <section>
      <h2>14. Appendix: Ch8.8.1 — Master Theorem</h2>

      <p>
        Many recursive algorithms have time complexity described by a recurrence
        of the form:
      </p>

      <pre><code class="language-cpp">
T(n) = a · T(n / b) + f(n)
      </code></pre>

      <p>
        The appendix teaches you how to solve such recurrences using the
        <strong>Master Theorem</strong>, with examples including mergesort and
        quicksort.
      </p>

      <p>
        You can read the full appendix here:
      </p>

      <p>
        <a href="/docs/08.algorithm/01.master/" class="next-button">
          Open Ch8.8.1: Master Theorem →
        </a>
      </p>
    </section>

    <section>
      <h2>Key takeaways</h2>

      <ul>
        <li><strong>&Theta;(n)</strong>: linear scans</li>
        <li><strong>&Theta;(n^2)</strong>: nested loops</li>
        <li><strong>O(ln&nbsp;n)</strong>: binary search</li>
        <li><strong>O(n&nbsp;ln&nbsp;n)</strong>: sorting</li>
        <li><strong>&Theta;(2^n)</strong>: naive Fibonacci</li>
        <li>Go and many NP-like problems have exponential search spaces</li>
        <li>AlphaGo succeeds by approximating, not solving exactly</li>
        <li>The Master Theorem (appendix) helps analyze recursive algorithms</li>
      </ul>
    </section>

    <script src="/sw-register.js"></script>
    <div class="page-navigation">
      <a href="/docs/08.algorithm/07.numeric/" class="prev-button">← Ch8.7: Numeric Algorithms</a>
      <a href="/" class="main-button">↑ Main Page</a>
      <a href="/docs/08.algorithm/09.rammachine/" class="next-button">Ch8.9: The RAM Model &amp; Its Limits →</a>
    </div>
  </main>
</body>
</html>
